{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC CODE FOR FRAUD DETECTION PROJECT IN AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary resources\n",
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker session, role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# S3 bucket name\n",
    "bucket = sagemaker_session.default_bucket()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset (given is one of the datasets)\n",
    "\n",
    "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c534768_creditcardfraud/creditcardfraud.zip\n",
    "!unzip creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv file\n",
    "local_data = 'creditcard.csv'\n",
    "\n",
    "# print out some data\n",
    "transaction_df = pd.read_csv(local_data)\n",
    "print('Data shape (rows, cols): ', transaction_df.shape)\n",
    "print()\n",
    "transaction_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of data points that are fraudulent\n",
    "def fraudulent_percentage(transaction_df):\n",
    "    '''Calculate the fraction of all data points that have a 'Class' label of 1; fraudulent.\n",
    "       :param transaction_df: Dataframe of all transaction data points; has a column 'Class'\n",
    "       :return: A fractional percentage of fraudulent data points/all points\n",
    "    '''\n",
    "    # counts for all classes\n",
    "    counts = transaction_df['Class'].value_counts()\n",
    "    \n",
    "    # get fraudulent and valid cnts\n",
    "    fraud_cnts = counts[1]\n",
    "    valid_cnts = counts[0]\n",
    "    \n",
    "    # calculate percentage of fraudulent data\n",
    "    fraud_percentage = fraud_cnts/(fraud_cnts+valid_cnts)\n",
    "    \n",
    "    return fraud_percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function to calculate the fraud percentage\n",
    "fraud_percentage = fraudulent_percentage(transaction_df)\n",
    "\n",
    "print('Fraudulent percentage = ', fraud_percentage)\n",
    "print('Total # of fraudulent pts: ', fraud_percentage*transaction_df.shape[0])\n",
    "print('Out of (total) pts: ', transaction_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTING TRAINING SET AND TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "def train_test_split(transaction_df, train_frac= 0.7, seed=1):\n",
    "    '''Shuffle the data and randomly split into train and test sets;\n",
    "       separate the class labels (the column in transaction_df) from the features.\n",
    "       :param df: Dataframe of all credit card transaction data\n",
    "       :param train_frac: The decimal fraction of data that should be training data\n",
    "       :param seed: Random seed for shuffling and reproducibility, default = 1\n",
    "       :return: Two tuples (in order): (train_features, train_labels), (test_features, test_labels)\n",
    "       '''\n",
    "    \n",
    "    # convert the df into a matrix for ease of splitting\n",
    "    df_matrix = transaction_df.as_matrix()\n",
    "    \n",
    "    # shuffle the data\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(df_matrix)\n",
    "    \n",
    "    # split the data\n",
    "    train_size = int(df_matrix.shape[0] * train_frac)\n",
    "    # features are all but last column\n",
    "    train_features  = df_matrix[:train_size, :-1]\n",
    "    # class labels *are* last column\n",
    "    train_labels = df_matrix[:train_size, -1]\n",
    "    # test data\n",
    "    test_features = df_matrix[train_size:, :-1]\n",
    "    test_labels = df_matrix[train_size:, -1]\n",
    "    \n",
    "    return (train_features, train_labels), (test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train/test data\n",
    "(train_features, train_labels), (test_features, test_labels) = train_test_split(transaction_df, train_frac=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual test\n",
    "\n",
    "# for a split of 0.7:0.3 there should be ~2.33x as many training as test pts\n",
    "print('Training data pts: ', len(train_features))\n",
    "print('Test data pts: ', len(test_features))\n",
    "print()\n",
    "\n",
    "# take a look at first item and see that it aligns with first row of data\n",
    "print('First item: \\n', train_features[0])\n",
    "print('Label: ', train_labels[0])\n",
    "print()\n",
    "\n",
    "# test split\n",
    "assert len(train_features) > 2.333*len(test_features), \\\n",
    "        'Unexpected number of train/test points for a train_frac=0.7'\n",
    "# test labels\n",
    "assert np.all(train_labels)== 0 or np.all(train_labels)== 1, \\\n",
    "        'Train labels should be 0s or 1s.'\n",
    "assert np.all(test_labels)== 0 or np.all(test_labels)== 1, \\\n",
    "        'Test labels should be 0s or 1s.'\n",
    "print('Tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELING\n",
    "ALGORITHM- LINEARLEARNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearLearner\n",
    "from sagemaker import LinearLearner\n",
    "\n",
    "# specify an output path\n",
    "prefix = 'creditcard'\n",
    "output_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "# instantiate LinearLearner\n",
    "linear = LinearLearner(role=role,\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='ml.c4.xlarge',\n",
    "                       predictor_type='binary_classifier',\n",
    "                       output_path=output_path,\n",
    "                       sagemaker_session=sagemaker_session,\n",
    "                       epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert features/labels to numpy\n",
    "train_x_np = train_features.astype('float32')\n",
    "train_y_np = train_labels.astype('float32')\n",
    "\n",
    "# create RecordSet\n",
    "formatted_train_data = linear.record_set(train_x_np, labels=train_y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "# train the estimator on formatted training data\n",
    "linear.fit(formatted_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# deploy and create a predictor\n",
    "linear_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELETEING ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(linear_predictor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p27",
   "language": "python",
   "name": "conda_amazonei_mxnet_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
